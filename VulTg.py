# coding:utf-8
"""
Created on Mar 9, 2021
@author: TBOsec   
"""
import json
import os
import re
from sys import exit

import httpx
from bs4 import BeautifulSoup

user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4495.0 Safari/537.36"
# æœºå™¨äººåœ°å€
webhook_addr = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=xxxx-xxxx"
id_url = "https://cloud.tencent.com/announce/ajax"
info_url = "https://cloud.tencent.com/announce/detail/"  # + id

# headerå¤´éƒ¨
getID_headers = {
    "User-Agent": user_agent,
    "Referer": "https://cloud.tencent.com/announce?page=1&categorys=21",
    "Content-Type": "application/json; charset=UTF-8",
    "Origin": "https://cloud.tencent.com",
}

getInfo_headers = {
    "User-Agent": user_agent,
    "Referer": "https://cloud.tencent.com/announce?page=1&categorys=21",
}

# æœºå™¨äººheaderå¤´éƒ¨
tx_headers = {
    "User-Agent": user_agent,
    "Content-Type": "application/json",
}

home = os.environ["HOME"]
dirs = home + "/.config/vul_TG/"
global page_id, total, titles, descriptions, old_total
page_id = []  # æ–‡ç« IDå·
titles = []  # æ–‡ç« æ ‡é¢˜
times = []  # é€šå‘Šæ—¶é—´
descriptions = []  # æ¼æ´é£é™©
risks = []  # é£é™©ç­‰çº§
versions = []  # å½±å“ç‰ˆæœ¬

# è·å–æ¼æ´IDå·
def getID():
    global page_id, total, old_total
    data = {
        "action": "getAnnounceList",
        "data": {"rp": 10, "page": 1, "categorys": ["21"], "labs": [], "keyword": ""},
    }
    with httpx.Client() as client:
        res = client.post(url=id_url, headers=getID_headers, json=data)
        res = json.loads(res.text)
        total = res["data"]["total"]
        # print(total)
        with open(dirs + "tx_id_history.log", "r+") as f:
            old_total = int(f.readlines()[0].strip())
            if total <= old_total:
                # print("ã€!ã€‘æš‚æ—¶æ— æ–°æ¼æ´ï¼ï¼ï¼")
                exit()
            else:
                content = f.read()
                f.seek(0, 0)
                f.write(str(total) + "\n" + content)
                # f.writelines(str(total))
                for i in range(10):
                    page_id.append(res["data"]["rows"][i]["announceId"])


# è·å–æ¼æ´è¯¦æƒ…
def getInfo():
    global page_id, total, titles, descriptions
    with httpx.Client() as client:
        for i in range(total - old_total):
            url = info_url + str(page_id[i])
            html_doc = httpx.get(url=url, headers=getInfo_headers)

            html_doc = html_doc.text
            soup = BeautifulSoup(html_doc, "lxml")

            # è·å–æ ‡é¢˜
            title = soup.find("h1").string
            titles.append(title)
            # é€šå‘Šæ—¶é—´
            date = soup.find(id="date").string
            times.append(date)

            all = re.compile(r"å½±å“ç‰ˆæœ¬.*?</div>(.*?)ä¿®å¤å»ºè®®", re.S)
            all = re.findall(all, html_doc)[0]
            version = ""
            if "å®‰å…¨ç‰ˆæœ¬" in all:
                if "æ’æŸ¥åŠæ³•" in all:
                    # print("1")
                    data1 = re.compile(r"(.*?)<span.*?æ’æŸ¥åŠæ³•", re.S)
                    data1 = re.findall(data1, all)[0]
                    soup = BeautifulSoup(data1, "lxml")
                    for info in soup.find("div").contents:
                        if str(info) == "<br/>":
                            version += "\n"
                        else:
                            version += info.string
                    versions.append(version)
                else:
                    # print("2")
                    data2 = re.compile(r"(.*?)å®‰å…¨ç‰ˆæœ¬", re.S)
                    data2 = re.findall(data2, all)[0]
                    soup = BeautifulSoup(data2, "lxml")
                    for info in soup.find("div").contents:
                        if str(info) == "<br/>":
                            version += "\n"
                        else:
                            version += info.string
                            # print(info.string, end="")
                    versions.append(version)

            # é£é™©ç­‰çº§
            risk = re.compile(r"é£é™©ç­‰çº§.*?bold;\">(.*?)</span>", re.S)
            risks.append(re.findall(risk, html_doc)[0])
            # æ¼æ´é£é™©
            description = re.compile(r"æ”»å‡»è€…åˆ©ç”¨(.*?)</div>", re.S)
            descriptions.append(re.findall(description, html_doc)[0])


# å‘é€@allä¿¡æ¯
# def send_all():
#     # global headers
#     data = {"msgtype": "text", "text": {"content": "", "mentioned_list": ["@all"]}}
#     with httpx.Client() as client:
#         client.post(webhook_addr, headers=tx_headers, json=data)

# æ ¼å¼åŒ–é€šå‘Šä¿¡æ¯
def format_data():

    for i in range(total - old_total):
        url = info_url + str(page_id[i])
        title = str(titles[i])
        # print(type(title))
        title = title.replace("ã€å®‰å…¨é€šå‘Šã€‘", "")  # æ›¿æ¢å¤šä½™çš„æ–‡å­—
        time = times[i]
        risk = risks[i]
        description = descriptions[i]
        version = versions[i]
        version = version.replace("\n\n", "\n")  # æ›¿æ¢æ‰å¤šä½™ç©ºè¡Œ
        # print(version)
        send_data = {
            "msgtype": "markdown",
            "markdown": {
                "content": "#### "
                + title
                + "\n>é€šå‘Šæ—¶é—´:"
                + time
                + "\n>é£é™©ç­‰çº§:`**"
                + risk
                + '**`\n>æ¼æ´é£é™©:<font color="comment">'
                + description
                + "</font>\n>å½±å“ç‰ˆæœ¬:\n`"
                + version
                + "`[æŸ¥çœ‹è¯¦æƒ…ğŸ”]("
                + url
                + ")"
            },
        }
        send_tg(send_data)


# å‘é€çš„ä¼å¾®æœºå™¨äºº
def send_tg(send_data):
    try:
        # with httpx.Client() as client:
        r = httpx.post(url=webhook_addr, headers=tx_headers, json=send_data)
        with open(dirs + "tx_history.log", "a") as f:
            f.writelines(r.text + "\n")

    except:
        exit()


def main():
    getID()
    getInfo()
    format_data()


if __name__ == "__main__":
    main()
